---
title: "Computational Musicology 2023"
author: "Juell Sprott"
output:
  flexdashboard::flex_dashboard:
    storyboard: true
    orientation: columns
    vertical_layout: fill
    theme: cosmo
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(plotly)
library(spotifyr)
library(reshape)
library(GGally)
library(compmus)
library(ggpubr)
library(ggplot2)
library(ggplotify)
library(gridExtra)

my_playlists <- readRDS(file = "data/artist_playlists.RDS")

```

Introduction
===================================================

This repository contains my course portfolio for Computational Musicology

PLEASE DO NOT SHOW THIS PORTFOLIO IN CLASS

For my corpus, I have decided to look into the various genres I have listened to for the past two years using Spotify Wrapped. From what I can see, I have listened to various genres like rock, pop, soul, media scores et cetera. However, one of the most occuring main genres to me is hip hop. As such, I have decided to create a corpus around rap, specifically using the artists I have listened to the most over the past two years.

For my corpus, I will be using the following artists:

- J. Cole
- Jay-Z
- Kid Cudi
- Kanye West
- Tyler, the Creator
- Vince Staples
- Swae Lee
- Meek Mill
- Mac Miller
- A$AP Rocky
- 21 Savage
- Childish Gambino
- Drake
- Metro Boomin
- Travis Scott
- Pop Smoke
- Frank Ocean

For these artists, their 'This Is' playlists from Spotify will be used.

I would like to investigate the difference in my 'bubble' in which my understanding and preference of rap songs goes, with some popular genres that are close in similarity to rap. For this, I will use some of Every Noise at Once's playlists with several (sub)genres chosen from the list based on their similarity to rap:

- southern hip hop
- conscious rap
- pop rap
- trap
- r&b
- melodic rap
- chicago rap
- g funk

The goal of this corpus is to create clusters within my corpus in order to analyze to which subgenres or styles these rappers belong to. This is done with the above genres.

First I will handle the artist section of my corpus. I believe the artists mentioned have created tracks for a long time, however they are mostly skewed towards more modern releases, rather than songs from say, before the 2000s. As such there is some lack of representation there. Also,  my list of artists only contains artist I have personally listened to. Generally speaking, most of these artist are quite popular worldwide, thus the section of my corpus lacks any of the less popular artists that are not popular outside of their home country or not popular at all. 

The second part of my corpus contains playlists obtained from Every Noise at Once. These contain songs that are much older than the first section, thus there is a bit more representation there. Likewise, less popular artists are also featured here. The problem of representation from songs written in different language still exists, however and I will likely have to look for ways to incorporate songs from other languages as well.


Visualization {.storyboard}
===================================================


### Comparing average features for each artists' playlist [track-level features]

```{r heatmap, echo=FALSE, warning=FALSE,message=FALSE}


playlist_heatmap <- readRDS(file = "data/playlist_heatmap.RDS")
playlist_heatmap

```

***
 
First, a heatmap in order to compare the average of certain features for the artists chosen ealier. The main goal is to get an idea of how the playlists compare on average. We can already identify several outliers, such as the playlists for Frank Ocean having overall lower scores for its features compared to other playlists. Jay-Z in comparison has much higher average energy and valence. Speech, valence and accousticness seem to have the most diversity.

### Comparing energy, accousticness, danceability and valence for all tracks [track-level features]

```{r, echo=FALSE}


parcoord <- readRDS(file = "data/parcoord.RDS")

ggplotly(parcoord)
```

***

Next, we dive deeper and take a look at the parallel coordinates for each playlist using various track-level features for every single track in our corpus. From this, we can see that the average distribution for features is somewhere along the line of high energy, low acousticness, high danceability and low valence, with some extreme outliers for Frank Ocean, Mac Miller and Vince Staples.

### Diving deeper into acousticness and valence  [track-level features]

```{r, echo=FALSE}

scatter <- readRDS(file = "data/scatter.RDS")
ggplotly(scatter)
```

***

We now do a comparison of two features in order to detect various track outliers. The majority of the songs seem to reside within low acousticness, while being somewhat evenly distributed in valence. The majority of artist playlist tracks fall under this, however each artist has tracks that deviate moderately from this norm. 
Some artists have a more even distribution among both valence and acousticness, while others have a majority of their tracks in high acousticness. 

### Comparing two outliers using chroma features [chroma features]

```{r, fig.width=15, echo=FALSE}

loud <- readRDS(file = "data/loud.RDS")
beach <- readRDS(file = "data/beach.RDS")

ggarrange(loud, beach, common.legend = TRUE)
```

***

For our chromagrams, we have chosen two outliers from the previous scatter plot. THE BEACH by Vince Staples and Loud by Mac Miller. For Loud, the song is mostly in the C and C#|Db keys, although in the first few seconds some time is spent in the F key. According to the previous scatter plot, this track has both low valence and accousticness. Thus, from these observations and the fact that a majority of songs in the corpus have low valence and acousticness, it can be said that there is a correlation between the low acousticness and valence and the C/C#|Db range of key notes.

THE BEACH on the other hand, is a lot more scattered, spending most of the time in G, F#|Gb, F and E keys, with occasional A and D key notes. This song has both high valence and acousticness. This, combined with the chromagram for Loud, shows us that valence and acousticness has a positive correlation with erraticness in key notes.

### Looking into temporal feature distribution of my corpus [temporal analysis]

```{r, fig.height=30, echo=FALSE}


tempo_box <- readRDS(file = "data/tempo_box.RDS")

ggplotly(tempo_box)

```

***

Here, a boxplot is made for each playlist, taking the tempo for each track. Here, we immediately see two outliers, namely Lord Pretty Flacko Jodye 2 for a BPM far higher than A$AP Rocky's other tracks and having the highest BPM. Meanwhile for lowest BPM, we have Tyler the Creator's BLESSED. Overall however, the boxplots seem to hover around 80 and 150 BPM, although the median for each artist differs greatly. Some artist seem to vary greatly in their BPMs, while others tend to stick around the same range.

### Inspecting the keys for different artist outliers in my corpus [tonal analysis]

```{r, echo=FALSE}


hist <- readRDS(file = "data/hist.RDS")

ggplotly(hist)
```

***

For this analysis, we will focus on four different artist with noticable difference compared to other artists in the corpus. These are Frank Ocean, Mac Miller, Tyler the Creator and Vince Staples.

As shown in the plots, the outlier playlist has most of its peaks in C, C#, F and G#. For all except Frank Ocean, C# is the most popular key. This makes sense as this is also a popular key for other playlists in my corpus. However, none of these artists have certain patterns in their histograms which they share with eachother. For example, Mac Miller tends to lean towards only 3 keys, while Tyler often branches out to different keys in his tracks.

### Looking in-depth into Runaway, by Kanye West [self-similarity matrices and cepstrograms]

```{r, echo=FALSE}


runaway <- readRDS(file = "data/runaway.RDS")
bind_rows(
  runaway |>
    compmus_self_similarity(pitches, "aitchison") |>
    mutate(d = d / max(d), type = "Chroma"),
  runaway |>
    compmus_self_similarity(timbre, "euclidean") |>
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")

```

***

One of the songs I believe is an outlier worth looking into is 'Runaway' by Kanye West, due to it's long duration and change in key notes throughout the song. Looking at the self-similarity matrices, we immediately notice the stark contrast between the first few seconds of the track and a moment after the 5 minute mark when compared to the rest of the song. When listening to the song, these moments are mostly made up of a paino solo, with one key repeatedly being pressed. Both matrices clearly show this difference, while also showing how the second contrasting moment splits the track into two distinct parts. Both parts build upon the piano, but contain different levels of instrumentalness and speech. This is easier seen in the self similarty matrix for chroma. When taking into acount the entire matrices however, the song seems to keep a clear structure throughout. 

### Classifying the various artist playlists using XGBoost and Every Noise At Once [playlist classification]

```{r,fig.width=20, echo=FALSE}


imp <- readRDS(file = "data/imp_ggplot.RDS")
his <- readRDS(file = "data/classify_hist.RDS")

ggarrange(imp, his)

```

***

Lastly, we will attempt to classify the artist playlists using some of the spotify features from the 8 mentioned subgenres, and attempt to look into if spotify features can accurately provide the genre of a track. Namely, we will be using "danceability", 
"speechiness", "acousticness", "instrumentalness", "liveness", "tempo", "energy", "loudness" and "valence" from the 8 subgenres chosen to be relatively similar to rap according to Every Noise At Once. Ideally, we would use some of the more in depth features, however due to the size of the data, using such features will computationally be quite expensive.
We will be using XGBoost as our multi-label classifier, using it's default values. After training our model, we obtain the ranking of most important features, and shown in the first plot. Tempo and speechiness seem to be most important for classifying the correct subgere, which makes sense as both tempo and speechiness can vary a lot between the various subgenres. As for prediction results, in the histogram, most tracks seem to be predicted as either G Funk, Melodic Rap or R&B. These three subgenres alone can encompass a large amount of the rap genre, thus it is no surprise that these labels are most predicted. Pop Rap and Trap seem to be least predicted, possibly due to the fact that both are hard to identify with purely the features provided, and would likely need other features to more accurately predict tracks for these labels. 

- Unfortunately, while trying to perform clustering, trying to group artist tracks into the various subgenres and visualize this accurately using t-SNE and heatmaps proved to either not provide meaningful results or was computationally complex to achieve. If possible, next time I would like to perform unsupervised learning that could give me better insight into the various genres of my corpus. 

Conclusion
===================================================

- With the usage of spotify's API and R, I was able to visualize various features of my corpus, ranging from simple track-level features to intricate timbre features using self-similarity matrices. I gained a better understanding of the various features that were hidden within my corpus, allowing me to understand what made some of these tracks unique to me and how I could possibly find tracks which could also interest me. Furthermore, I gained a lot of knowledge on effectively using R for data visualization, feature selection and modelling. Prior to this course, I had mostly limited myself to simple Python plots, however R has shown me how much I could improve plots to present to others.

- I would have liked to spend more time looking into clustering and classification techniques, as I felt I did not spend as much time or effort as I should have. Perhaps I could perform some of the future in order to look for artists I may also be interested in.